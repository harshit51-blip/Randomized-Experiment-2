Some thoughts on the previous version of this code (basic write up is titled 'Randomized Periodic Walks') covered under the repository titled 'Randomized-Experiment':
It was akin to taking a jar of particles whole and randomly placing it in a room. It was a random walk but with no real content, albeit a good learning curve.
I came back to what I originally wanted to do: simulate a system with simple evolution and bring a random change in a particle of the system. 
I have highlighted the basic setup of the system in the pdf available in this repository. I had to change the way V_x is initialized, 
instead of doing it at the start of every iteration, do it at the end after any and all changes to both V of previous iteration and X of the current iteration.
I do not have any other thoughts about the basic setup of the system at this moment. Below I will be going through the 3 main observations I thought were interesting:


1) Just to compare with the previous version of the system, I plotted the arithmetic mean of list V at any given iteration, against the iteration. I do not really know what I was   expecting but the results I got were fascinating. A constantly increasing straight line with little deviations. The value of the mean seems to be related directly to the number   of iterations; for 10^n iterations, the end mean seems to be around 5 x 10^(n-1). I do not know what to make of this constantly increasing straight line. Graphs are provided.
2) I also plotted the V_d of each iteration; the difference between the highest and the lowest V(x) in each iteration. At first the results weren't anything encouraging. However     on closer inspection, it seems the increase in V_d each iteration depends on the number of particles in list X, i.e., in the simulation. As the number of particles increases,     the graph starts taking the shape of a logarithmic graph with deviations. I will upload many graphs showcasing this. I believe this could simply be because I am plotting           difference, obviously difference between 10 particles will tend to be more than difference in 2 particles (in a finitely divisible space, i.e., finite precision). However, that   increase in difference seems to take the shape of a logarithmic curve. I thought it might have something to do with the exponential nature of perturbation rule I chose. But even   after removing that part of the code and running the it, the pattern remained.
3) I had the idea of the plotting the difference in the V_d, known as dV_d, of each iteration. Just to see how V_d changes. dV_d was a graph filled with periodic lines within a     seemingly well-defined region on the y-axis, only the external perturbations made dV_d go outside this region. I found it very similiar to the behaviour of a random walk. A       particle choosing to go in a direction of certain length periodically. I checked this by doing the same for the my previous version of the code. And the results were the same.     Thus, I think I can confidently conclude that there some sort of a random walk similar to the previous one, is inside this simulation.

These are all the condensed observations for now, I will add updates. I will also write a paper on the simulation soon and do a thorough mathematical analysis of the model.
To try and explain these rather interesting results I was not expecting. 
I have some ideas on how to approach the results. As the graphs show, they approach a definite curve as the data set (number of particles and iterations) increases. Thus, we can look at the data and ask what would happen if the data was continuous, i.e., a very large data set with an arbitrary precision. We can analyse this sort of a continuous stream of data, which inherently means that there is an X and a V(x) associated to every point on the 1 dimensional array of arbitrary precision, whatever that means. Instead of looking at discrete data and explaining it, we can then work with continuous data and work toward discreteness. It may be obvious to someone working with physics, Maths and CS but it was an insight to me.

Update: I was thinking about how both the Mean vs Iterations and V_d vs Iterations seem to be dependent on the number of particles in the system. That got me thinking about a possible relationship between Mean and V_d. I plotted Mean vs V_D and V_d vs Mean and the results were very fascinating.  V_d vs Mean is very (almost identical) to V_d vs Iterations. And when Mean is on the y-axis, i.e., Mean vs V_d, it's just the reverse of the V_d vs Mean graph. So, there does seem to be a fascinating relationship. I really do not know how and where to proceed. A mathematical analysis is critical.

I also have confidence that this behaviour is directly related to the variable y, the random variable chosen to evolve the position of a particle. I know this since when I took it out, everything fell flat.

Update: I was thinking about this simulation in general and I realized that for mean, I was using V(x), and since it was a function of sine, that could explain why it tends to a straight line. I thought I should graph a 'Mean of X' and I did and I have uploaded it (From now on, every graph with no indication whether it is a Mean of X or V(x), it should be taken as V(x). But from now on, I will explicitly mention it). Coming to Mean of X vs Iteration, it showed a clearly exponential graph with the random Perturbation ON and OFF. I have no explanation for this. The smoothness of the curve may point to the fact that some approximation is happening somewhere. The end value is always of the form 2.5 x 10^(n+something) where n is exponent of the Iterations, of the form 10^n.
